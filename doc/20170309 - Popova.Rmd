---
title: "20170309 - Popova Model"
output: html_notebook
---

Better to do experimental stan fitting in R. Copying from the ipynb:

cf. <a href=https://scholar.google.com/citations?view_op=view_citation&hl=en&user=0BVB3sIAAAAJ&citation_for_view=0BVB3sIAAAAJ:eQOLeE2rZwMC>Popova, Popova, & George, 2008, Bayesian Forecasting of Prepayment Rates for Individual Pools of Mortgages</a>.

Endo is defined as the log of Actual Payment, which is the difference in UPB in a given period. In other words, it includes both prepayment and expected decline in principle.

Betas are:
* incentive: WAC - prevailing mortgage rate, as before
* spline: incentive cubed
* burnout: log ratio of pool_upb to scheduled balance, $B_t$
    * $B_t =  B_0 * \frac{(1 + \tfrac{c}{12})^n - (1+\tfrac{c}{12})^t}{(1+\tfrac{c}{12})^n-1}$, where t=WALA, c=WAC, and n=WARM+WAC (should be 360)
* seasonality: 1 if it's the summer-time, i.e. May-August.
* yield_curve: 10Y rate minus 2Y rate

I'm also using a filtered dataset that only includes GNM II Single Family mortgages. There are about 45,000 pools in this dataset.

They estimate as a mixed effects model. I don't see any provision for the differing loan sizes of these pools. But I'll fit it as it looks in their paper first.

```{r, message=FALSE, warning=FALSE}
library(knitr)
opts_knit$set(root.dir = "~/src/LondonMirror/Prepayments/")
setwd("~/src/LondonMirror/Prepayments/")
library(tidyverse)
library(rstan)
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())

sample_data = read_csv("~/src/LondonMirror/Prepayments/popova_data.csv")
#Scale it
sample_data <- sample_data %>% mutate(incentive = incentive * 50, 
                                      spline = spline * 1.3e4, 
                                      yc = yc * 2)
```

```{r stan_code}
stan_code = "data {
    int N; #Number of records
    int K; #number of distributions in the mixture
    int B; #number of betas
    vector[N] y; #log(actual_payments)
    row_vector[N] sp; #log(scheduled_payments)
    matrix[N,B] x; #factors: incentive, spline, burnout, season, yc
}
parameters {
    row_vector[B] beta[K]; #betas for each mixture
    real<lower=0> sigma[K]; #scale for y
    simplex[K] p; #mixture proportions
}
transformed parameters {
    row_vector[N] mu[K]; 
    for(k in 1:K) {
        mu[k] = sp .* (beta[k] * x');
    }
}
model {
    real ps[K]; #Temp for log component densities
    for(k in 1:K) {
        beta[k] ~ normal(0,10);
    }
    p ~ dirichlet(rep_vector(0.5,K));
    for(n in 1:N) {
        for(k in 1:K) {
            ps[k] = log(p[k]) + normal_lpdf(y[n] | mu[k,n], sigma[k]);    
        }
        target += log_sum_exp(ps);
    }
}
"
```

```{r}
betas <- c("incentive","spline","burnout","seasonality","yc")
fit_data =list(N=dim(sample_data)[1], K=2, B=5, y=sample_data$log_endo,
                      sp=sample_data$log_sp, x=as.matrix(sample_data[,betas]))
fit <- stan(data=fit_data, model_code = stan_code, model_name="popova_mixed2")
```

Not fitting well. First mixture gets no weight, causing divergent transitions. Try a different prior on `p`.

Let's look at the endo distribution again.
```{r}
sample_data %>% ggplot(aes(x=log_endo)) + geom_histogram()
```

That's pretty clear. But I fitted with an intercept.
```{r}
sample_data %>% ggplot(aes(x=log_endo - log_sp)) + geom_histogram()
```

Ok. So let's not get fancy. fit the intercept as a modal effect. But then isn't the mixed model just looking at different initial pool sizes?

```{r}
stan_code = "data {
    int N; #Number of records
    int K; #number of mixtures. It's 2.
    int B; #number of betas
    vector[N] y; #log(actual_payments)
    matrix[N,B] x; #factors: incentive, spline, burnout, season, yc
}
parameters {
    row_vector[B] beta[K]; #betas for each mixture
    real<lower=0> sigma[K]; #scale for y
    real intercept[K];
}
transformed parameters {
    row_vector[N] mu[K]; 
    for(k in 1:K) {
        mu[k] = intercept[k] + (beta[k] * x');
    }
}
model {
    real ps[K]; #Temp for log component densities
    for(k in 1:K) {
        beta[k] ~ normal(0,10);
    }
    intercept ~ normal(0,10);

    for(n in 1:N) {
        ps[1] = log(0.67) + normal_lpdf(y[n] | mu[1,n], sigma[1]);    
        ps[2] = log(0.33) + normal_lpdf(y[n] | mu[2,n], sigma[2]);    
        target += log_sum_exp(ps);
    }
}
"
betas <- c("incentive","spline","burnout","seasonality","yc")
fit_data <- list(N=dim(sample_data)[1], K=2, B=5,
                 y=sample_data$log_endo-sample_data$log_sp, 
                 sp=sample_data$log_sp, x=as.matrix(sample_data[,betas]))

fit <- stan(data=fit_data, model_code = stan_code, model_name="popova_mixed2",
            pars=c("mu"), include=FALSE)
```

Hooray! Not divergent. But these f'ckers will not mix properly. There's always one chain that gets stuck in the opposite regime to the others. I'm going to excise it. It's chain 3.

```{r}
params <- extract(fit, pars=c("beta", "intercept", "sigma"), permuted=FALSE)
params <- rbind(params[,1,],params[,2,],params[,4,]) #Screw you, chain 3!
median_params <- apply(params,2,median)
fit
```
...and we still have a negative beta on `incentive`. Not that that should be surprising given the look of the scatter plot.

I guess I'd better move on. I'll try sampling all historical data for a given pool, regressing on that.